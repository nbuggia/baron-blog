title: Managing Search Engine Access to Your Site

<strong>Note: Originally published on http://janeandrobot.com</strong>

Controlling what content is blocked from being found in search engines is crucial for many websites. Fortunately, the major search engines and other well-behaved robots observe the <a href="http://www.robotstxt.org/" target="_blank">Robots Exclusion Protocol</a> (REP), which has evolved organically since the early 1990′s to provide a set of controls over what parts of a web site search engines robots can crawl and index.

Article Sections:
<ul>
	<li><a href="#Capabilities_of_the_REP">Capabilities of REP</a></li>
	<li><a href="#Deciding_what_should_be_Public_vs._Private">Deciding What Should be Public vs. Private</a></li>
	<li><a href="#Implementing_the_REP">Implementing the REP</a>
<ul style="margin-bottom: 0pt;">
	<li><a href="#Site_Level_Implementation_(Robots.txt)">Site Level</a></li>
	<li><a href="#Page_Level_Implementation_(META_Tags)">Page Level (Meta Tags)</a></li>
	<li><a href="#HTTP_Header_Implementation_(X-ROBOTS-Tag)">Page Level (HTTP Header)</a></li>
	<li><a href="#Content_Level_Implementation">Content Level</a></li>
</ul>
</li>
	<li><a href="#Common_implementation_mistakes">Common Mistakes</a></li>
	<li><a href="#Testing_your_implementation_">Testing Your Implementation</a></li>
	<li><a href="#removal">Removing Content From Search Engine Indices</a></li>
	<li><a href="#Additional_Resources:_">Additional Resources</a></li>
</ul>
<h2><a name="Capabilities_of_the_REP"></a>Capabilities of the REP</h2>
The Robots Exclusion Protocol provides controls that can be applied at the site level (robots.txt), at the page level (META tag, or X-Robots-Tag), or at the HTML element level to control both the crawl of your site and the way it’s listed in the search engine results pages (SERPs). Below is a table listing the common scenarios, directives, and which search engines support them.
<table style="border-width: 1px !important;">
<tbody>
<tr>
<td valign="top"><strong>Use Case</strong></td>
<td style="text-align: center;" valign="top"><strong>Robots.txt</strong></td>
<td style="text-align: center;" valign="top"><strong>META/ X-Robots-Tag</strong></td>
<td style="text-align: center;" valign="top"><strong>Other</strong></td>
<td style="text-align: center;" valign="top"><strong>Supported By</strong></td>
</tr>
<tr>
<td valign="top">Allow access to your content</td>
<td style="text-align: center;" valign="top">Allow</td>
<td style="text-align: center;" valign="top">FOLLOWINDEX</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40364">Google</a><a href="http://help.yahoo.com/l/us/yahoo/search/webcrawler/slurp-02.html">Yahoo</a><a href="http://blogs.msdn.com/webmaster/archive/2008/06/03/robots-exclusion-protocol-joining-together-to-provide-better-documentation.aspx">Microsoft</a></td>
</tr>
<tr>
<td valign="top">Disallow access to your content</td>
<td style="text-align: center;" valign="top">Disallow</td>
<td style="text-align: center;" valign="top">NOINDEXNOFOLLOW</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><a href="http://www.google.com/support/webmasters/bin/answer.py?hl=en&amp;answer=35303">Google</a><a href="http://help.yahoo.com/l/us/yahoo/search/webcrawler/slurp-02.html">Yahoo</a><a href="http://blogs.msdn.com/webmaster/archive/2008/06/03/robots-exclusion-protocol-joining-together-to-provide-better-documentation.aspx">Microsoft</a></td>
</tr>
<tr>
<td valign="top">Disallow access to index images on the page</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top">NOIMAGEINDEX</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><a href="http://www.google.com/support/webmasters/bin/answer.py?hl=en&amp;answer=79892">Google</a></td>
</tr>
<tr>
<td valign="top">Disallow the display of a cached version of your content in the SERP</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top">NOARCHIVE</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=35306=">Google</a><a href="http://help.yahoo.com/l/us/yahoo/search/deletion/basics-10.html">Yahoo</a><a href="http://blogs.msdn.com/webmaster/archive/2008/06/03/robots-exclusion-protocol-joining-together-to-provide-better-documentation.aspx">Microsoft</a></td>
</tr>
<tr>
<td valign="top">Disallow the creation of a description for this content in the SERP</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top">NOSNIPPET</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=35304">Google</a><a href="http://www.ysearchblog.com/archives/000587.html">Yahoo</a><a href="http://blogs.msdn.com/webmaster/archive/2008/06/03/robots-exclusion-protocol-joining-together-to-provide-better-documentation.aspx">Microsoft</a></td>
</tr>
<tr>
<td valign="top">Disallow the translation of your content into other languages</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top">NOTRANSLATE</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><a href="http://www.google.com/help/faq_translation.html#donttrans">Google</a></td>
</tr>
<tr>
<td valign="top">Do not follow or give weight to links within this content</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top">NOFOLLOW</td>
<td style="text-align: center;" valign="top">a href attribute:rel=NOFOLLOW</td>
<td style="text-align: center;" valign="top"><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=96569">Google</a><a href="http://www.ysearchblog.com/archives/000069.html">Yahoo</a><a href="http://blogs.msdn.com/livesearch/archive/2005/01/18/nofollow_tags.aspx">Microsoft</a></td>
</tr>
<tr>
<td valign="top">Do not use the <a href="http://www.dmoz.org/" target="_blank">Open Directory Project</a> (ODP) to create descriptions for your content in the SERP</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top">NOODP</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=35264">Google</a><a href="http://help.yahoo.com/l/us/yahoo/search/indexing/indexing-11.html">Yahoo</a><a href="http://blogs.msdn.com/webmaster/archive/2008/06/03/robots-exclusion-protocol-joining-together-to-provide-better-documentation.aspx">Microsoft</a></td>
</tr>
<tr>
<td valign="top">Do not use the Yahoo Directory to create descriptions for your content in the SERP</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><span style="font-family: 'Tahoma','sans-serif'; font-size: 10pt;">NOYDIR</span></td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><a href="http://blogs.msdn.com/webmaster/archive/2008/06/03/robots-exclusion-protocol-joining-together-to-provide-better-documentation.aspx">Yahoo</a></td>
</tr>
<tr>
<td valign="top">Do not index this specific element within an HTML page</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top">class=robots-nocontent</td>
<td style="text-align: center;" valign="top"><a href="http://www.ysearchblog.com/archives/000444.html">Yahoo</a></td>
</tr>
<tr>
<td valign="top">Stop indexing this content after a specific date</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top">UNAVAILABLE_AFTER</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><a href="http://googleblog.blogspot.com/2007/07/robots-exclusion-protocol-now-with-even.html">Google</a></td>
</tr>
<tr>
<td valign="top">Disallow the creation of enhanced captions</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top">NOPREVIEW</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><a href="http://bing.com/community">Microsoft</a></td>
</tr>
<tr>
<td valign="top">Specify a sitemap file or a sitemap index file</td>
<td style="text-align: center;" valign="top">Sitemap</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><a href="http://www.google.com/support/webmasters/bin/answer.py?hl=en&amp;answer=64748">Google</a><a href="http://www.ysearchblog.com/archives/000437.html">Yahoo</a><a href="http://blogs.msdn.com/livesearch/archive/2007/04/11/discovering-sitemaps.aspx">Microsoft</a></td>
</tr>
<tr>
<td valign="top">Specify how frequently a crawler may access your website</td>
<td style="text-align: center;" valign="top">Crawl-Delay</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><a href="http://google.com/webmaster">Google WMT</a></td>
<td style="text-align: center;" valign="top"><a href="http://help.yahoo.com/l/us/yahoo/search/webcrawler/slurp-03.html">Yahoo</a><a href="http://blogs.msdn.com/webmaster/archive/2008/04/18/ramping-up-msnbot.aspx">Microsoft</a></td>
</tr>
<tr>
<td valign="top">Authenticate the identity of the crawler</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top">Reverse DNS Lookup</td>
<td style="text-align: center;" valign="top"><a href="http://googlewebmastercentral.blogspot.com/2006/09/how-to-verify-googlebot.html">Google</a><a href="http://www.ysearchblog.com/archives/000460.html">Yahoo</a><a href="http://blogs.msdn.com/livesearch/archive/2006/11/29/search-robots-in-disguise.aspx">Microsoft</a></td>
</tr>
<tr>
<td valign="top">Request removal of your content from the engine’s index</td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"></td>
<td style="text-align: center;" valign="top"><a href="http://google.com/webmaster">Google WMT</a><a href="http://siteexplorer.search.yahoo.com">Yahoo SE</a><a href="http://webmaster.live.com/">Microsoft WMT</a></td>
<td style="text-align: center;" valign="top"><a href="http://googlewebmastercentral.blogspot.com/2007/04/requesting-removal-of-content-from-our.html">Google</a><a href="http://help.yahoo.com/l/us/yahoo/search/siteexplorer/delete/">Yahoo</a>Microsoft</td>
</tr>
</tbody>
</table>
<h2><a name="Deciding_what_should_be_Public_vs._Private"></a>Deciding What Should be Public vs. Private</h2>
One of the first steps in managing the robots is knowing what type of content should be public vs. private. Start with the assumption that by default, everything is public, then explicitly identify the items that are private.

If you want search engines to access all the content on your site, you don’t need a robots.txt file at all. When a search engine tries to access the robots.txt file on your site and the server can’t return one (ideally by returning a 404 HTTP status code), the search engine treats this the same as a robots.txt file that allows access to everything.

Every website and every business has a different set of needs, so there’s no blanket rule for what to make private, but some common elements may apply.
<ul>
	<li><strong>Private data</strong>– You may have content on your site that you don’t want to be searchable in search engines. For instance, you may have private user information (such as addresses) that you don’t want surfaced. For this type of content, you may want to use a more secure approach that keeps all visitors from the pages (such as password protection). However, some types of content are fine for visitor access, but not search engine access. For instance, you may run a discussion forum that is open for public viewing, but you may not want individual posts to appear in search results for forum member names.</li>
	<li><strong><a name="noncontent"></a>Non-content content</strong> – Some content, like <a href="/post/Effectively-Using-Images.aspx#noncontent">images used for navigation</a>, provides little value to searchers. It’s not harmful to include these items in search engine indices, but since search engines allocate limited bandwidth to crawl each site and limited space to store content from each site, it may make sense to block these items to help direct the bots to the content on your site that you do want indexed.</li>
	<li><strong>Printer-friendly pages</strong>– if you have specific pages (URLs) that are formatted for printing you may want to block them out to avoid duplicate content issues. The drawback to allowing the printer-friendly page to be indexed is that it could potentially be listed in the search results instead of the default version of the page, which wouldn’t provide an ideal user experience for a visitor coming to the site through search.</li>
	<li><strong>Affiliate links and advertising</strong>– If you include advertising on your site, you can keep search engine robots from following the links by redirecting them to a blocked page, then on to the destination page. (There are other methods for implementing advertising-based links as well.)</li>
	<li><strong>Landing pages</strong>– Your site may include multiple variations of entry pages used for advertising purposes. For instance, you may run AdWords campaigns that link to a particular version of a page based on the ad, or you may print different URLs for different print ad campaigns (either for tracking purposes or to provide a custom experience related to the ad). Since these pages are meant to be an extension of the ad, and are generally near duplicates of the default version of the page, you may want to block these landing pages from being indexed.</li>
	<li><strong>Experimental pages</strong> – As you try new ideas on your site (for instance, using A/B testing), you likely want to block all but the original page from being indexed during the experiment.</li>
</ul>
<h2><a name="Implementing_the_REP"></a>Implementing the REP</h2>
REP is flexible and can be implemented a number of ways. This flexibility lets you easily specify some policies for your entire site (or subdomain) and then enhance them more granularly at the page or link level as needed.
<h3><a name="Site_Level_Implementation_(Robots.txt)"></a>Site Level Implementation (Robots.txt)</h3>
Site wide directives are stored in a robots.txt file, which must be located in the root directory of each domain or sub-domain (e.g. <a href="/robots.txt">http://janeandrobot.com/robots.txt</a>.) Note that robots.txt files only apply to the hostname where they are placed, and do not apply to subdomains. So a robots.txt file located on <a href="http://microsoft.com/robots.txt">http://microsoft.com/robots.txt</a> will not apply to the MSDN subdomain <a href="http://msdn.microsoft.com/">http://msdn.microsoft.com</a>. However, the robots.txt file does apply to all subfolders and pages within the specified hostname.

A robots.txt file is a UTF-8 encoded file that contains entries that consist of a user-agent line (that tells the search engine robot if the entry is directed at it) and one or more directives that specify content that the search engine robot is blocked from crawling or indexing. A simple robots.txt file is shown below.
<div >
<pre class="alt">User-agent: *</pre>
<pre class="alt">Disallow: /private</pre>
</div>
<code>user-agent:</code> – Specifies which robots the entry applies to.
<ul>
	<li>Set this to <code>*</code>to specify that this entry applies to all search engine robots.</li>
	<li>Set this to a specific robot name to provide instructions for just that robot. You can find a complete list of robot names at <a href="http://www.robotstxt.org">robotstxt.org</a>.</li>
	<li>If you direct an entry at a particular robot, then it obeys that entry <em>instead</em> of any entries defined for <code>user-agent: * </code>(rather than in addition to those entries).</li>
</ul>
The major search engines have multiple robots that crawl the web for different types of content (such as images or mobile). They generally begin all robots with the same name so that if you block the major robot, all robots for that search engine are blocked as well. However, if you want to block only the more specific robot, you can block it directly and still allow web crawl access.
<ul>
	<li><strong><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40364">Google</a></strong>– The primary search engine robot is Googlebot.</li>
	<li><strong><a href="http://help.yahoo.com/l/us/yahoo/search/webcrawler/slurp-02.html">Yahoo!</a></strong>– The primary search engine robot is Slurp.</li>
	<li><strong><a href="http://blogs.msdn.com/livesearch/archive/2006/11/29/search-robots-in-disguise.aspx">Live Search</a></strong> – The primary search engine robots is MSNbot.</li>
</ul>
<code>Disallow: </code>- Specifies what content is blocked
<ul>
	<li>Must begin with a slash (<code>/</code>).</li>
	<li>Blocks access to any URLs that begin with the characters after the <code>/</code>. For instance, <code>Disallow: /images</code> blocks access to <code>/images/</code>, <code>/images/image1.jpg</code>, and <code>/images10</code>.</li>
</ul>
You can specify other rules for search engine robots in addition to the standard instructions that block access to content as noted in <a href="#other">other robot instructions</a>.

Some things to note about robots.txt implementation:
<ul>
	<li>The major search engines support pattern matching using the asterisk character (*) for wildcard match and the dollar sign ($) for end of sequence matching as described below in <a href="#patterns">using pattern matching</a>.</li>
	<li>The robots.txt file is case sensitive, so <code>Disallow: /images </code>would block <code>http://www.example.com/images</code> but not <code>http://www.example.com/Images</code>.</li>
	<li>If conflicts exist in the file, the robot obeys the longest (and therefore generally more specific) line.</li>
</ul>
<h4>Basic Samples</h4>
<em><strong>Block all robots</strong></em>

Useful when your site is in pre-launch development and isn’t ready for search traffic.
<div >
<pre class="alt"># This keeps out all well-behaved robots.</pre>
<pre class="alt"># Disallow: * is not valid.</pre>
<pre class="alt">User-agent: *</pre>
<pre class="alt">Disallow: /</pre>
<pre class="alt"></pre>
</div>
<em><strong>Keep out all bots by default</strong></em>

Blocks all pages except those specified. Not recommended as is difficult to maintain and diagnose.
<div >
<pre class="alt"># Stay out unless otherwise stated</pre>
<pre class="alt">User-agent: *</pre>
<pre class="alt">Disallow: /</pre>
<pre class="alt">Allow: /Public/</pre>
<pre class="alt">Allow: /articles/</pre>
<pre class="alt">Allow: /images/</pre>
<pre class="alt"></pre>
</div>
<strong><em>Block specific content</em> </strong>

The most common usage of robots.txt.
<div >
<pre class="alt"># Block access to the images folder</pre>
<pre class="alt">User-agent: *</pre>
<pre class="alt">Disallow: /images/</pre>
<pre class="alt"></pre>
</div>
<a name="allow"></a><strong><em>Allow specific content</em> </strong>

Block a folder, but allow access to selected pages in that folder.
<div >
<pre class="alt"># Block everything in the images folder</pre>
<pre class="alt"># Except allow images/image1.jpg</pre>
<pre class="alt">User-agent: *</pre>
<pre class="alt">Disallow: /images/</pre>
<pre class="alt">Allow: /images/image1.jpg</pre>
<pre class="alt"></pre>
</div>
<em><strong>Allow specific robot</strong></em>

Block a class of robots (for instance, Googlebot), but allow a specific bot in that class (for instance, Googlebot-Mobile).
<div >
<pre class="alt"># Block Googlebot access</pre>
<pre class="alt"># Allow Googlebot-Mobile access</pre>
<pre class="alt">User-agent: Googlebot</pre>
<pre class="alt">Disallow: /</pre>
<pre class="alt">User-agent: Googlebot-Mobile</pre>
<pre class="alt">Allow: /</pre>
<pre class="alt"></pre>
</div>
<h4>Pattern Matching Examples</h4>
The major engines support two types of pattern matching.
<ul>
	<li><strong>*</strong>matches any sequence of characters</li>
	<li><strong>$</strong> matches the end of URL</li>
</ul>
<em><strong>Block access to URLs that contain a set of characters</strong></em>

Use the asterisk (*) to specify a wildcard.
<div >
<pre class="alt"># Block access to all URLs that include an ampersand</pre>
<pre class="alt">User-agent: *</pre>
<pre class="alt">Disallow: /*&amp;</pre>
<pre class="alt"></pre>
</div>
This directive would block search engines from crawling <code>http://www.example.com/page1.asp?id=5&amp;sessionid=xyz</code>.

<strong><em>Block access to URLs that end with a set of characters</em> </strong>

Use the dollar sign ($) to specify end of line.
<div class="answer_heading">
<div >
<pre class="alt"># Block access to all URLs that end in .cgi</pre>
<pre class="alt">User-agent: *</pre>
<pre class="alt">Disallow: /*.cgi$</pre>
<pre class="alt"></pre>
</div>
This directive would block search engines from crawling <code>http://www.example.com/script1.cgi</code> but <em>not</em> from crawling <code>http://www.example.com/script1.cgi?value=1</code>.

<em><strong>Selectively allow access to a URL that matches a blocked pattern</strong></em>

Use the <code>Allow</code> directive in conjunction with pattern matching for more complex implementations.
<div >
<pre class="alt"># Block access to URLs that contain ?</pre>
<pre class="alt"># Allow access to URLs that end in ?</pre>
<pre class="alt">User-agent: *</pre>
<pre class="alt">Disallow: /*?</pre>
<pre class="alt">Allow: /*?$</pre>
<pre class="alt"></pre>
</div>
That directive blocks all URLs that contain <code>?</code> except those that end in <code>?</code>. In this example, the default version of the page will be indexable:
<ul>
	<li><code>http://www.example.com/productlisting.aspx?</code></li>
</ul>
Variations of the page will be blocked:
<ul>
	<li><code>http://www.example.com/productlisting.aspx?nav=price</code></li>
	<li><code>http://www.example.com/productlisting.aspx?sort=alpha</code></li>
</ul>
<h4><a name="other"></a>Other robot instructions</h4>
</div>
<em><strong><span class="style2">Specify a Sitemap or Sitemap index file</span> </strong></em>

If you’d like to provide search engines with a comprehensive list of your best URLs, you can provide one or more <a href="http://sitemaps.org" target="_blank">Sitemap</a> autodiscovery directives. Note, user-agent does not apply to this directive so you cannot use this to specify a Sitemap to some but not all search engines.
<div >
<pre class="alt"># Please take my sitemap and index everything!</pre>
<pre class="alt">Sitemap: <a href="http://janeandrobot.com/sitemap.axd">http://janeandrobot.com/sitemap.axd</a></pre>
<pre class="alt"></pre>
</div>
<strong><em>Reduce the crawling load</em> </strong>

This only works with Microsoft and Yahoo. For Google you’ll need to specify a slower crawling speed through their <a href="http://google.com/webmaster" target="_blank">Webmaster Tools</a>. Be careful when implementing this because if you slow down the crawl too much, robots won’t be able to get to all of your site and you may lose pages from the index.
<div >
<pre class="alt"># MSNBot, please wait 5 seconds in between visits</pre>
<pre class="alt">User-agent: msnbot</pre>
<pre class="alt">Crawl-delay: 5</pre>
&nbsp;
<pre class="alt"># Yahoo's Slurp, please wait 12 seconds in between visits</pre>
<pre class="alt">User-agent: slurp</pre>
<pre class="alt">Crawl-delay: 12</pre>
<pre class="alt"></pre>
</div>
<h3><a name="Page_Level_Implementation_(META_Tags)"></a>Page Level Implementation (META Tags)</h3>
The REP page-level directives allow you to refine the site wide policies on a page-by-page basis

<strong><em>Placing a meta tag on the page</em> </strong>

Place the meta tag in the head tag. Each directive should be comma delimited inside the tag. E.g. &lt;meta name=”ROBOTS” content=”Directive1, Directive 2&gt;.
<div >
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">html</span><span class="kwrd">&gt;</span></pre>
<pre class="alt"><span class="kwrd"> &lt;</span><span class="html">head</span><span class="kwrd">&gt;</span></pre>
<pre class="alt"><span class="kwrd"> &lt;</span><span class="html">title</span><span class="kwrd">&gt;</span>Your title here<span class="kwrd">&lt;/</span><span class="html">title</span><span class="kwrd">&gt;</span></pre>
<pre class="alt"><span class="kwrd"> &lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOINDEX"</span><span class="kwrd">&gt;</span></pre>
<pre class="alt"><span class="kwrd"> &lt;/</span><span class="html">head</span><span class="kwrd">&gt;</span></pre>
<pre class="alt"><span class="kwrd"> &lt;</span><span class="html">body</span><span class="kwrd">&gt;</span>Your page here<span class="kwrd">&lt;/</span><span class="html">body</span><span class="kwrd">&gt;</span></pre>
<pre class="alt"><span class="kwrd">&lt;/</span><span class="html">html</span><span class="kwrd">&gt;</span></pre>
<pre class="alt"></pre>
</div>
<strong><em>Targeting a specific search engine</em> </strong>

Within the meta tag you can specify which search engine you would like to target, or you can target them all.
<div >
<pre class="alt"><span class="rem">&lt;!-- Applies to All Robots --&gt;</span></pre>
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOINDEX"</span><span class="kwrd">&gt;</span></pre>
&nbsp;
<pre class="alt"><span class="rem">&lt;!-- ONLY GoogleBot --&gt;</span></pre>
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="Googlebot"</span> <span class="attr">content</span><span class="kwrd">="NOINDEX"</span><span class="kwrd">&gt;</span></pre>
&nbsp;
<pre class="alt"><span class="rem">&lt;!-- ONLY Slurp (Yahoo) --&gt;</span></pre>
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="Slurp"</span> <span class="attr">content</span><span class="kwrd">="NOINDEX"</span><span class="kwrd">&gt;</span></pre>
&nbsp;
<pre class="alt"><span class="rem">&lt;!-- ONLY MSNBot (Microsoft) --&gt;</span></pre>
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="MSNBot"</span> <span class="attr">content</span><span class="kwrd">="NOINDEX"</span><span class="kwrd">&gt;</span></pre>
</div>
<em>Control how your listings</em> – there are a set of options you can use to determine how your site will show up on the SERP. You can exert some control over how the description is created, and remove the “Cached page” link.
<p style="text-align: center;"><img class="aligncenter size-full wp-image-85" style="border: black 1px solid;" title="example-serp" alt="example-serp" src="/images/articles/rep-example-serp.gif" /></p>

<div >
<pre class="alt"><span class="rem">&lt;!-- Do not show a description for this page --&gt;</span></pre>
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOSNIPPET"</span><span class="kwrd">&gt;</span></pre>
&nbsp;
<pre class="alt"><span class="rem">&lt;!-- Do not use http://dmoz.org to create a description --&gt;</span></pre>
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOODP"</span><span class="kwrd">&gt;</span></pre>
&nbsp;
<pre class="alt"><span class="rem">&lt;!-- Do not present a cached version of the document in a search result --&gt;</span></pre>
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOARCHIVE"</span><span class="kwrd">&gt;</span></pre>
<pre class="alt"></pre>
</div>
<em><strong>Using other directives</strong></em>

Other meta robots directives are shown below.
<div >
<pre class="alt"><span class="rem">&lt;!-- Do not trust links on this page, could be user generated content (UCG) --&gt;</span></pre>
&nbsp;
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOFOLLOW"</span><span class="kwrd">&gt;</span></pre>
<pre class="alt"><span class="rem">&lt;!-- Do not index this page --&gt;</span></pre>
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOINDEX"</span><span class="kwrd">&gt;</span></pre>
&nbsp;
<pre class="alt"><span class="rem">&lt;!-- Do not index any images on this page (will still index the if they are linked</span></pre>
<pre class="alt"><span class="rem"> elsewhere) Better to use Robots.txt if you really want them safe.</span></pre>
<pre class="alt"><span class="rem"> This is a Google Only tag. --&gt;</span></pre>
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="GOOGLEBOT"</span> <span class="attr">content</span><span class="kwrd">="NOIMAGEINDEX"</span><span class="kwrd">&gt;</span></pre>
&nbsp;
<pre class="alt"><span class="rem">&lt;!-- Do not translate this page into other languages--&gt;</span></pre>
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOTRANSLATE"</span><span class="kwrd">&gt;</span></pre>
&nbsp;
<pre class="alt"><span class="rem">&lt;!-- NOT RECOMMENDED, there really isn't much point in using these --&gt;</span></pre>
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="FOLLOW"</span><span class="kwrd">&gt;</span></pre>
<pre class="alt"><span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOT"</span> <span class="attr">content</span><span class="kwrd">="UNAVAILABLE_AFTER"</span><span class="kwrd">&gt;</span></pre>
<pre class="alt"></pre>
</div>
<h3><a name="HTTP_Header_Implementation_(X-ROBOTS-Tag)"></a>HTTP Header Implementation (X-ROBOTS-Tag)</h3>
Allows developers to specify page-level REP directives for non text/html content types like PDF, DOC, PPT, or dynamically generated images.

<strong><em>Using the X-Robots-Tag</em> </strong>

Use the X-Robots-Tag, simply add it to your header as shown below. To specify multiple directives you can either comma delimit them, or add them as separate header items.
<div >
<pre class="alt">HTTP/1.x 200 OK</pre>
<pre class="alt">Cache-Control: private</pre>
<pre class="alt">Content-Length: 2199552</pre>
<pre class="alt">Content-Type: application/octet-stream</pre>
<pre class="alt">Server: Microsoft-IIS/7.0</pre>
<pre class="alt">content-disposition: inline; filename=01 - The truth about SEO.ppt</pre>
<pre class="alt"><strong>X-Robots-Tag: noindex, nosnippet</strong></pre>
<pre class="alt">X-Powered-By: ASP.NET</pre>
<pre class="alt">Date: Sun, 01 Jun 2008 19:25:47 GMT</pre>
</div>
&nbsp;

The X-Robots-Tag directive supports most of the same directives as the meta tag. The only limitation with this method over the meta tag implementation is that there is no way to target a specific robot – though that probably isn’t a big deal for most use cases.
<ul>
	<li><span style="font-family: courier new;">X-Robots-Tag: noindex</span></li>
	<li><span style="font-family: courier new;">X-Robots-Tag: nosnippet</span></li>
	<li><span style="font-family: courier new;">X-Robots-Tag: notranslate</span></li>
	<li><span style="font-family: courier new;">X-Robots-Tag: noarchive</span></li>
	<li><span style="font-family: courier new;">X-Robots-Tag: unavailable_after: 7 Jul 2007 16:30:00 GMT</span></li>
</ul>
<h3><a name="Content_Level_Implementation"></a>Content Level Implementation</h3>
You can further refine your site level and page level directives within several content tags.

Each anchor tag (link) can be modified to tell search engines that you do not trust where this URL is pointing to. This is typically used for links within user generated content (UCG) like wikis, blog comments, reviews and other community sites.
<div >
<pre class="alt">&lt;a href="#" rel="NOFOLLOW"&gt;My Hyperlink&lt;/a&gt;</pre>
<pre class="alt"></pre>
</div>
Also, in Yahoo Search you can specify which &lt;div&gt; elements on a page you would not like indexed using the <code>class=robots-nocontent</code> attribute. However, we don’t highly recommend using this tag because it is not supported in any other engine, making it not super-useful.
<div >
<pre class="alt">&lt;div class="robots-nocontent"&gt;</pre>
<pre class="alt">No content for you! (or at least Yahoo!)</pre>
<pre class="alt">&lt;/div&gt;</pre>
<pre class="alt"></pre>
</div>
<h2><a name="Common_implementation_mistakes"></a>Common Mistakes</h2>
While implementing the REP is generally straight-forward, there are a few common mistakes.

<strong><em>GoogleBot follows the most specific directive, ignoring all others</em> </strong>

In the robots.txt file, if you specify a section for all user-agents (<code>user-agent: *</code>) and also declare a section for Googlebot (<code>user-agent: Googlebot</code>), Google will disregard all sections in the robots.txt file except the Googlebot section. This could potentially leave you exposing much more content to Google that you might have thought.
<div >
<pre class="alt"># This keeps out all well-behaved robots</pre>
</div>
<div >
<pre class="alt">User-agent: *</pre>
</div>
<div >
<pre class="alt">Disallow: /</pre>
</div>
<div >
<pre class="alt"># This looks like it is giving Google access to only this directory, but since it is a</pre>
</div>
<div >
<pre class="alt"># GoogleBot specific section, Google will disregard the previous section</pre>
</div>
<div >
<pre class="alt"># and access the whole site.</pre>
</div>
<div >
<pre class="alt">User-agent: Googlebot</pre>
</div>
<div >
<pre class="alt">Allow: /Content_For_Google/</pre>
<pre class="alt"></pre>
</div>
<strong><em>NOFOLLOW will most likely not prevent indexing</em> </strong>

if you use <code>NOFOLLOW</code> at either the page or the link level, it is still possible for the links from the page to be indexed because the search engine may have found a reference to them from another source. Another note, using <code>rel="NOFOLLOW"</code> within your anchor text is still perceived as a recommendation by the search engines, not a command.To ensure that content is not indexed, either use the <code>Disallow</code> directive at the site level, or use <code>NOINDEX</code> at the page level.

<strong><em>Directives that are not recommended</em> </strong>

Directives in the REP are all about exceptions, by default the robots assume they can crawl your whole site. Therefore, you do not need to explicitly use the <code>FOLLOW</code> and <code>INDEX</code> directives as they will not be taken into account by the search engines. It sounds silly but I’ve seen a few sites that have implemented these on every page and every link.Another directive that is not recommended is the <code>NOCACHE</code> directive. This was created by Microsoft, and is synonymous with <code>NOARCHIVE</code>. While they will most likely always continue to support the directive, it is better to use <code>NOARCHIVE</code> so it will work on all the search engines.

<strong><em>Be cognizant of case</em> </strong>

When referencing files and URLs in the Robots.txt file, use a defensive approace to URL case, as the major engines do not handle it the same way. (e.g. /Files does not always equal /files).
<h2><a name="Testing_your_implementation_"></a>Testing Your Implementation</h2>
As you’re implementing your REP design, you should test it both before you deploy it and after. The easiest way to test this is to use the robots validator in either Google or Microsoft’s Webmaster Tools. These tools are generally good enough test beds for most folks, however advanced developers (or paranoid ones with critical business requirements) will want to definitively know what the robots are doing, not simply rely on what the robots say they are doing. These folks will want to look at their tools as well look at their server logs.

In addition to using validation tools, reporting tools from the search engines on what they couldn’t acces, and looking at logs data to see what the search engine robots are crawling, you should check the search engine results to see if any pages you are intending to block are being indexed. If they are, use the methods described in this section to ensure you are blocking them correctly and <a href="#removal">use the search engine tools to request that the pages be removed</a>.

<a name="partial"></a><strong><em>When Blocked Content Appears to be Indexed</em> </strong>

If search engines are blocked from crawling pages, they may still index the URL if the robot finds a link to that URL on a page that isn’t blocked. The listing may display the URL only, such as shown below.
<p style="text-align: center;"><img class="aligncenter size-full wp-image-84" title="urlonly" alt="urlonly" src="/images/articles/rep-url-only.gif" /></p>
Or, it may include a title and in some instances, a description. This makes it appear as though the search engine robot is disregarding the directive that blocks access to the page, but the search engine is in fact obeying the directive not to crawl the page and is using anchor text from the link to that page and descriptive details from either the page that contains the link or a source such as the <a href="http://www.dmoz.org">Open Directory Project</a>.

For more details, see:
<ul>
	<li><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=35667">Google: partially indexed page</a></li>
	<li><a href="http://help.yahoo.com/l/us/yahoo/search/webcrawler/slurp-01.html">Yahoo!: thin documents</a></li>
</ul>
<h3><a name="The_Easy_Way_"></a>The Easy Way</h3>
Both Google and Microsoft provide some tools as part of their Webmaster Centers to help you verify if you’ve configured your REP the way you expect. Let’s start with Google’s tools:

The first thing you should check are the list of URLs that Google has seen from your website and not indexed due to the REP. Note you can also download the list and filter, sort, and have-your-way-with-it in Excel.
<p style="text-align: center;"><img class="aligncenter size-full wp-image-83" style="border: black 1px solid;" title="webmaster-robotstxt-blocked1" alt="webmaster-robotstxt-blocked1" src="/images/articles/rep-webmaster-robotstxt-blocked1.gif" /></p>
The next step is to use their interactive robots.txt tool to analyze your rules and test specific URLs for blockage. When you pull up the tool they already should have it pre-populated with the robots.txt file they have on file from the last time they crawled. You can input a list of URLs you’d like to check below, select the user-agent you’d like to check against and the tool will tell you if they are blocked or not. You can also use the tool to test changes to your robots.txt file to see how Google would interpret things.
<p style="text-align: center;"><img class="aligncenter size-full wp-image-81" style="border: black 1px solid;" title="google-analyze-robotstxt" alt="google-analyze-robotstxt" src="/images/articles/rep-google-analyze-robotstxt.jpg" /></p>
Microsoft has a similar tool in their <a href="http://webmaster.live.com/">Webmaster Center</a> that will validate a robots.txt file against the standard that MSNBot supports. To use the tool, simply log in copy &amp; paste your robots.txt file into the top field and select <strong>Validate</strong>. A list of all detectable issues are displayed in the bottom box.
<p style="text-align: center;"><img class="aligncenter size-full wp-image-79" style="border: black 1px solid;" title="microsoft-robotstxt-validat" alt="microsoft-robotstxt-validat" src="/images/articles/rep-microsoft-robotstxt-validat.jpg" /></p>

<h3><a name="The_Hard_Way_(More_Accurate)"></a>The Hard Way</h3>
<strong><em>More Accurate Views of Robot Access Through Your Logs</em> </strong>

If you have a specific business need to ensure that the robots are following your rules, (or you’re just paranoid) then you should not simply rely on the tools they provide to test compliance. You’re going to need to go straight to the horse’s mouth and analyze your web server logs to see exactly what they are doing. There is no one easy tool for doing this, you’ll likely have to use an existing tool like one of these (<a href="http://www.microsoft.com/downloads/details.aspx?FamilyID=890cd06b-abf8-4c25-91b2-f8d975cf8c07">Microsoft HTTP Log Parser</a>) or write your own. It isn’t difficult, it will simply take some time to implement. A useful reference for this is a list of all the robot <a href="http://www.robotstxt.org/db.html">user agents</a>, and more complete list of bots from <a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40364">Google</a>, and <a href="http://blogs.msdn.com/livesearch/archive/2006/11/29/search-robots-in-disguise.aspx">Microsoft</a>.

<a name="verify"></a><em><strong>Verifying Robot Identity</strong></em>

Another thing you’ll likely want to consider in this endeavor is to validate that the robots are who they actually say they are. Google, Yahoo and Microsoft all support <a href="http://en.wikipedia.org/wiki/Reverse_DNS_lookup">Reverse DNS authentication</a> of their robots. The process is pretty simple and described here by <a href="http://googlewebmastercentral.blogspot.com/2006/09/how-to-verify-googlebot.html">Google</a>, <a href="http://www.ysearchblog.com/archives/000460.html">Yahoo </a>and <a href="http://blogs.msdn.com/livesearch/archive/2006/11/29/search-robots-in-disguise.aspx">Microsoft</a>, essentially you simply find out what range their robot’s DNS is hosted in, and use that in your tool. This way, if the address changes (which it will), you don’t need to update your code.

Should you find any issues, where one of the robots are not minding the REP, or are misbehaving in some other way, you can always communicate directly with each engine through one of their forums:
<ul>
	<li><a href="http://groups.google.com/group/Google_Webmaster_Help-Indexing/topics">Google Crawling, Indexing and Ranking Forum</a></li>
	<li><a href="http://help.yahoo.com/l/us/yahoo/search/search_support.html">Yahoo Crawler Feedback Form</a></li>
	<li><a href="http://forums.microsoft.com/webmaster/ShowForum.aspx?ForumID=1984&amp;SiteID=79">Microsoft Crawler Error and Feedback Forum</a></li>
</ul>
<h2><a name="removal"></a>Removing Content From Search Engine Indices</h2>
If you find that you haven’t implemented the techniques described here correctly and private content from your site is indexed, each of the major search engines has methods available for requesting that it be removed. For more information, see:
<ul>
	<li><a href="http://googlewebmastercentral.blogspot.com/2007/04/requesting-removal-of-content-from-our.html">Google: Requesting removal of content from our index</a></li>
	<li><a href="http://help.yahoo.com/l/us/yahoo/search/siteexplorer/delete/">Yahoo!: Deleting URLs</a></li>
	<li><a href="https://support.live.com/eform.aspx?productKey=wlsearch&amp;page=wlsupport_home_options_form_byemail&amp;ct=eformts">Live Search: Requesting content removal</a></li>
</ul>
<h2><a name="Additional_Resources:_"></a>Additional Resources:</h2>
<ul>
	<li>Google
<ul>
	<li><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40362">How to create a robots.txt file</a></li>
	<li><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40364">Descriptions of each user-agent that Google uses</a></li>
	<li><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40367">How to use pattern matching</a></li>
	<li><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40368">How often we recrawl your robots.txt file</a></li>
	<li><a href="http://googlewebmastercentral.blogspot.com/2006/08/all-about-googlebot.html">All about Googlebot</a></li>
</ul>
</li>
	<li>Yahoo!
<ul>
	<li><a href="http://www.ysearchblog.com/archives/000372.html">Wild card support</a></li>
	<li><a href="http://www.ysearchblog.com/archives/000508.html">X-Robots tag directive support</a></li>
</ul>
</li>
	<li>Microsoft Live Search
<ul>
	<li><a href="http://blogs.msdn.com/livesearch/archive/2006/11/29/search-robots-in-disguise.aspx">Search robots in disguise</a></li>
</ul>
</li>
	<li>Other resources
<ul>
	<li><a href="http://searchengineland.com/070305-204850.php">Search Engine Land: Meta Robots Tag 101</a></li>
	<li><a href="http://searchengineland.com/080603-121100.php">Search Engine Land: Yahoo!, Microsoft, Google Clarify Robots.txt Support</a></li>
	<li><a href="http://searchengineland.com/070417-213813.php">Search Engine Land: URL Removal Options</a></li>
	<li><a href="http://www.robotstxt.org/">robotstxt.org</a></li>
	<li><a href="http://en.wikipedia.org/wiki/Robots.txt">Wikipedia: Robots Exclusion Standard</a></li>
</ul>
</li>
</ul>
<h3>Revision History</h3>
<ul>
	<li>02/12/2009 – Google, Yahoo and Microsoft make a joint announcement of the rel=’Canonical’ tag to make it easier for publishers to specify the canonical URLs.</li>
	<li>06/04/2009 – Added NOPREVIEW tag announced this week by Microsoft. Used to disable the ‘hover preview’ feature on their SERP.</li>
</ul>